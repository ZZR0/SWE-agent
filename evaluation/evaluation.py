import argparse
import json
import os
import traceback
import logging
import os
import shutil
import subprocess
from multiprocessing import Pool
from collections import Counter

from swebench import (
    KEY_INSTANCE_ID,
    KEY_MODEL,
    KEY_PREDICTION,
    get_eval_report,
    get_logs_eval,
    get_model_report,
    get_resolution_status,
    get_eval_refs,
)
from swebench.harness.constants import INSTALL_FAIL
from swebench.harness.utils import get_instances

from unidiff import PatchSet

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("evaluation")

WORK_DIR = "/data1/zengzhengran/sweTrans_yang"

def validate_predictions(predictions_path, tasks_ids):
    # æ£€æŸ¥é¢„æµ‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not any([predictions_path.endswith(x) for x in [".json", ".jsonl"]]):
        raise ValueError("Predictions path must be .json or .jsonl file")
    predictions = get_instances(predictions_path) # åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œæ¯ä¸ªå­—å…¸å½¢å¦‚{'model_name_or_path': 'vllm-llama3-70b', 'instance_id': 'pvlib__pvlib-python-1072', 'model_patch': '\ndiff --git a/pvlib/temperature.py ...'}
    not_in_tasks = []
    # æ£€æŸ¥é¢„æµ‹æ˜¯å¦æ­£ç¡®æ ¼å¼åŒ–
    for pred in predictions:
        if any([x not in pred for x in [KEY_INSTANCE_ID, KEY_MODEL, KEY_PREDICTION]]):
            raise ValueError(f"Every prediction must have {KEY_INSTANCE_ID}, {KEY_MODEL}, and {KEY_PREDICTION} fields")
        if pred[KEY_INSTANCE_ID] not in tasks_ids:
            not_in_tasks.append(pred[KEY_INSTANCE_ID])
    # Check that instance IDs specified by predictions exist
    if len(not_in_tasks) > 0:
        logger.warning(
            "Predictions for the following instance_ids were not "
            + "found in the tasks file and will not be considered: "
            + ", ".join(not_in_tasks)
        )

def eval_engine_docker(args):
    # instance_id = f"{item['instance_id']}"
    # if not "pyvista__pyvista-4315" in instance_id:
    #     return
    args.log_dir = args.log_dir.replace(f"{WORK_DIR}", "/") # æŠŠå·¥ä½œç›®å½•æ›¿æ¢æˆ/ï¼Œå› åé¢dockerè¦åšæ˜ å°„ï¼Œå³æŠŠå·¥ä½œç›®å½•æ˜ å°„åˆ°å®¿ä¸»æœº
    args.temp_dir = args.temp_dir.replace(f"{WORK_DIR}", "/")
    args.predictions_path = args.predictions_path.replace(f"{WORK_DIR}", "/")
    image_name = f"zzr/swe-env--{args.repo.replace('/', '__')}__{args.version}"
    cmd = f"""
        docker run --rm -it \
        --network host -e ALL_PROXY=http://192.168.100.211:10809 \
        -v {WORK_DIR}/SWE-agent:/SWE-agent \
        -v {WORK_DIR}/SWE-bench:/SWE-bench \
        {image_name} \
        python /SWE-agent/evaluation/engine_evaluation.py \
            --path_conda /root/miniconda3 \
            --testbed /testbed \
            --num_workers 1 \
            --log_dir {args.log_dir} \
            --predictions_path {args.predictions_path} \
            --temp_dir {args.temp_dir} \
            --timeout {args.timeout} \
    """
    if args.verbose: # æ˜¯å¦æ˜¾ç¤ºæ—¥å¿—
        cmd += " --verbose"
    if args.skip_existing: # æ˜¯å¦è·³è¿‡å·²ç»å­˜åœ¨æ—¥å¿—çš„é¢„æµ‹ç»“æœçš„è¯„ä¼°
        cmd += " --skip_existing"
    cmd = " ".join(cmd.strip().split())
    logger.info("==="*10)
    logger.info(cmd)
    logger.info("==="*10)
    os.system(cmd)

def run_evaluation(
    predictions_path: str,
    swe_bench_tasks: str,
    log_dir: str,
    testbed: str,
    conda_link: str,
    log_suffix: str,
    skip_existing: bool,
    timeout: int,
    verbose: bool,
    num_processes: int = -1,
    path_conda: str = None,
):
    """
    å¯¹æ¯ä¸ªæ¨¡å‹/åº“/ç‰ˆæœ¬ç»„åˆçš„é¢„æµ‹ç»“æœè¿è¡Œè¯„ä¼°ã€‚

    Args:
        predictions_path (str): Path to the predictions file.
        swe_bench_tasks (str): Path to the SWE-bench tasks file OR HF dataset name.
        log_dir (str): ä¿å­˜æ—¥å¿—çš„ç›®å½•è·¯å¾„ã€‚
        testbed (str): ä¿å­˜æµ‹è¯•ç»“æœçš„ç›®å½•è·¯å¾„ã€‚
        skip_existing (bool): æ˜¯å¦è·³è¿‡å·²ç»å­˜åœ¨æ—¥å¿—çš„é¢„æµ‹ç»“æœçš„è¯„ä¼°ã€‚
        timeout (int): æ¯ä¸ªè¯„ä¼°çš„è¶…æ—¶æ—¶é—´ã€‚
        verbose (bool): æ˜¯å¦æ‰“å°è¯¦ç»†è¾“å‡ºã€‚
        path_conda (str): conda ç¯å¢ƒæ–‡ä»¶çš„è·¯å¾„ã€‚

    Raises:
        ValueError: å¦‚æœ log_dir ä¸æ˜¯ç›®å½•ï¼Œtestbed ä¸æ˜¯ç›®å½•ï¼Œæˆ– swe_bench_tasks ä¸å­˜åœ¨ã€‚
    """
    # éªŒè¯å‚æ•°
    if not os.path.exists(log_dir) or not os.path.isdir(log_dir):
        raise ValueError("--log_dir must exist and point at a directory")
    if not os.path.exists(testbed) or not os.path.isdir(testbed):
        raise ValueError("--testbed must exist and point at a directory")
    
    tasks = list(get_eval_refs(swe_bench_tasks).values())

    # éªŒè¯å‚æ•°çš„æ ¼å¼æ˜¯å¦æ­£ç¡®
    if not isinstance(tasks, list):
        raise ValueError(f"{swe_bench_tasks} must contain an array of tasks")
    tasks_map = {t[KEY_INSTANCE_ID]: t for t in tasks} # å­—å…¸ï¼Œæ ¼å¼å½¢å¦‚{'pyvista__pyvista-4315': {'instance_id': 'pyvista__pyvista-4315', 'model': 'pyvista', 'version': '4315'}, ...}
    predictions_path = os.path.abspath(predictions_path) # è·å–ç»å¯¹è·¯å¾„
    validate_predictions(predictions_path, [t[KEY_INSTANCE_ID] for t in tasks]) # æ£€æŸ¥æ˜¯å¦æœ‰éæ³•æ ¼å¼ï¼Œæ˜¯å¦æœ‰predictionsæ— æ³•å¯¹åº”tasks

    # æŒ‰æ¨¡å‹å¯¹é¢„æµ‹è¿›è¡Œåˆ†ç»„
    predictions = get_instances(predictions_path)
    logger.info(f"Found {len(predictions)} predictions in predictions file")

    # For each model, split predictions by repo + save to folder
    eval_args = [] # ä¿å­˜æ‰€æœ‰å‚æ•°çš„åˆ—è¡¨
    temp_dirs = [] # ä¿å­˜æ–‡ä»¶å¤¹è·¯å¾„çš„åˆ—è¡¨ï¼Œæ–‡ä»¶å¤¹å½¢å¦‚'/data1/zengzhengran/sweTrans_yang/SWE-agent/evaluation/testbed/pvlib__pvlib-python-1072'
    for p in predictions:
        # Group predictions by repository, version
        repo = p[KEY_INSTANCE_ID].rsplit("-", 1)[0]
        t = tasks_map[p[KEY_INSTANCE_ID]]
        p.update(t) # å°†tä¸­çš„é”®å€¼å¯¹æ·»åŠ åˆ°pä¸­
        version = t["version"]

        # åˆ›å»ºé’ˆå¯¹instance_idçš„testbedæ–‡ä»¶å¤¹
        testbed_save_dir = os.path.join(testbed, p[KEY_INSTANCE_ID])
        os.makedirs(testbed_save_dir, exist_ok=True)

        # åˆ›å»ºç”¨äºå­˜å‚¨model/repo/versionçš„é¢„æµ‹æ–‡ä»¶
        file_name = f"{predictions_path.split('/')[-1]}"
        file_path = os.path.join(testbed_save_dir, file_name)
        if file_path.endswith(".jsonl"):
            file_path = file_path[:-1] # æŠŠjsonlæ”¹ä¸ºjson

        # Create evaluation args
        args = argparse.Namespace()
        args.repo = repo
        args.version = version
        args.log_dir = log_dir
        args.log_suffix = log_suffix
        args.num_workers = 1
        args.predictions_path = file_path
        args.skip_existing = skip_existing
        args.temp_dir = testbed_save_dir
        args.timeout = timeout
        args.verbose = verbose
        args.conda_link = conda_link
        args.path_conda = path_conda

        # Save predictions to file
        with open(file_path, "w") as f:
            json.dump([p], f, indent=4)

        eval_args.append(args)
        temp_dirs.append(testbed_save_dir)

    eval_args = eval_args[1:2] # æå–ç´¢å¼•ä¸º1çš„å…ƒç´ ï¼Œå½¢å¦‚[Namespace(repo='sqlfluff__sqlfluff', version='0.6', log_dir='/data1/zengzhengran/sweTrans_yang/SWE-agent/evaluation/log', log_suffix=None, num_workers=1, predictions_path='/data1/zengzhengran/sweTrans_yang/SWE-agent/evaluation/testbed/sqlfluff__sqlfluff-1625/all_preds_filtered.json', skip_existing=True, temp_dir='/data1/zengzhengran/sweTrans_yang/SWE-agent/evaluation/testbed/sqlfluff__sqlfluff-1625', timeout=900, verbose=True, conda_link=None, path_conda=None)]
    temp_dirs = temp_dirs[1:2] # æå–ç´¢å¼•ä¸º1çš„å…ƒç´ 
    if len(eval_args) == 0: # æ²¡æœ‰é¢„æµ‹ç»“æœ
        logger.info("No predictions to evaluate")
        return

    # Run evaluation on each model/repo
    # å¦‚æœnum_processeså¤§äº0ï¼Œåˆ™é€‰æ‹©è¾ƒå°çš„å€¼ï¼Œä»¥ç¡®ä¿ä¸è¶…è¿‡eval_argsçš„é•¿åº¦
    # å¦åˆ™ï¼Œnum_processesç­‰äºeval_argsçš„é•¿åº¦
    num_processes = min(len(eval_args), num_processes) if num_processes > 0 else len(eval_args)
    try:
        if num_processes == 1:
            for args in eval_args:
                eval_engine_docker(args)
        else:
            pool = Pool(processes=num_processes)
            pool.map(eval_engine_docker, eval_args)
            pool.close()
            pool.join()
    finally:
        # Clean up
        for temp_dir in temp_dirs:
            # Kill all processes that are using the temp directory
            subprocess.run(f"lsof +D {temp_dir} | awk 'NR>1 {{print $2}}' | xargs kill", shell=True, capture_output=True)
            # Remove temp directory
            # shutil.rmtree(temp_dir, ignore_errors=True) # åˆ æ‰tmpæ–‡ä»¶å¤¹

def main(predictions_path, log_dir, swe_bench_tasks, testbed, 
         skip_existing, timeout, verbose, conda_link, 
         log_suffix, num_processes, path_conda, instance_filter):
    # Check if paths exist
    if not os.path.exists(predictions_path):
        raise FileNotFoundError(f"Predictions path {predictions_path} does not exist")
    eval_refs = get_eval_refs(swe_bench_tasks)
    for k, v in eval_refs.items():
        eval_refs[k] = {key: v[key] for key in [KEY_INSTANCE_ID, "FAIL_TO_PASS", "PASS_TO_PASS"]}
    """
    è½¬æ¢åçš„æ ¼å¼
        eval_refs = {'sqlfluff__sqlfluff-1625': 
        {'instance_id': 'sqlfluff__sqlfluff-1625', 
         'FAIL_TO_PASS': ['test/cli/commands_test.py::test__cli__command_directed'], 
         'PASS_TO_PASS': ['test/cli/commands_test.py::test_encoding[utf-32-UTF-32]', ...]
         },}
    """

    # Change model_name_or_patch field to directory name for all predictions
    directory = os.path.dirname(predictions_path) # è·å–ç»™å®šæ–‡ä»¶è·¯å¾„çš„ç›®å½•è·¯å¾„
    directory_name = directory.rsplit("/", 1)[-1] # é€šè¿‡å°†ç›®å½•è·¯å¾„æŒ‰ç…§"/"è¿›è¡Œæ‹†åˆ†ï¼Œè·å–ç›®å½•çš„åç§°
    pred_path_orig = predictions_path # å¤‡ä»½åŸå§‹çš„æ–‡ä»¶è·¯å¾„
    pred_path_temp = predictions_path.replace(".jsonl", "_filtered.jsonl") # å°†åŸå§‹æ–‡ä»¶è·¯å¾„ä¸­çš„".jsonl"æ›¿æ¢ä¸º"_filtered.jsonl"ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„æ–‡ä»¶è·¯å¾„

    pred_total, pred_will_eval = 0, 0 # åˆå§‹åŒ–é¢„æµ‹æ€»æ•°å’Œå°†è¦è¯„ä¼°çš„é¢„æµ‹æ•°ä¸º0
    with open(pred_path_temp, "w") as f:
        for l in open(pred_path_orig, "r").readlines():
            pred_total += 1
            p = json.loads(l)
            # æ’é™¤é¢„æµ‹ç»“æœä¸ºç©ºå­—ç¬¦ä¸²çš„æƒ…å†µï¼Œå†™å…¥_filtered.jsonlä¸­
            if p[KEY_PREDICTION] is not None and p[KEY_PREDICTION].strip() != "":
                p[KEY_MODEL] = directory_name
                json.dump(p, f)
                f.write("\n")
                pred_will_eval += 1
    logger.info(
        f"Found {pred_total} total predictions, will evaluate {pred_will_eval} ({pred_total-pred_will_eval} are empty)"
    )

    # Run evaluation
    predictions_path = pred_path_temp
    try:
        logger.info("ğŸƒ Beginning evaluation...")
        run_evaluation(
            predictions_path=predictions_path,
            log_dir=log_dir,
            swe_bench_tasks=swe_bench_tasks,
            testbed=testbed,
            skip_existing=skip_existing,
            timeout=timeout,
            verbose=verbose,
            conda_link=conda_link,
            log_suffix=log_suffix,
            num_processes=num_processes
        )
        logger.info("âœ… Finished evaluation")
    except Exception as e:
        logger.info(f"âŒ Evaluation failed: {e}\n{traceback.format_exc()}")
    logger.info("==================================")
    # os.remove(pred_path_temp)

    # Get predictions, define log_dir
    predictions = [json.loads(l) for l in open(pred_path_orig, "r").readlines()] # è·å–é¢„æµ‹ç»“æœï¼Œå°†æ¯è¡Œå­—ç¬¦ä¸²è§£æä¸ºJSONå¯¹è±¡ï¼Œå¹¶å­˜å‚¨åœ¨åˆ—è¡¨predictionsä¸­
    logger.info(f"Log directory for evaluation run: {log_dir}") # ä½¿ç”¨loggerè®°å½•æ—¥å¿—ï¼Œæ‰“å°åŒ…å«è¯„ä¼°è¿è¡Œæ—¥å¿—ç›®å½•çš„æ¶ˆæ¯

    # Iterate through predictions
    scorecards = [] # æ€»ä½“ç»“æœ
    for p in predictions:
        scorecard = {KEY_INSTANCE_ID: p[KEY_INSTANCE_ID], "statuses": [], "stats": {}}

        # å¦‚æœå­˜åœ¨traj_pathï¼Œæ·»åŠ trajectoryç»Ÿè®¡ä¿¡æ¯
        traj_path = os.path.join(directory, f"{p[KEY_INSTANCE_ID]}.traj")
        if os.path.exists(traj_path):
            traj_data = json.load(open(traj_path, "r")) # åŠ è½½æ•°æ®
            scorecard["stats"]["traj_num_steps"] = len(traj_data["trajectory"]) # è®°å½•"trajectory"çš„é•¿åº¦
            scorecard["stats"]["traj_action_dist"] = dict( # ç»Ÿè®¡"history"ä¸­"assistant"è§’è‰²çš„actionçš„åˆ†å¸ƒæƒ…å†µ
                Counter( # ç»Ÿè®¡roleä¸ºassistantçš„actionçš„æƒ…å†µï¼Œç»“æœå½¢å¦‚{'create': 5, 'edit': 2}
                    [
                        entry["action"].strip().split()[0] # åªå–ç¬¬ä¸€ä¸ªè¯ï¼ŒåŠ¨è¯ï¼Œå½¢å¦‚'create'ï¼Œ'edit'
                        if entry["role"] == "assistant" and "action" in entry and len(entry["action"]) > 0
                        else None
                        for entry in traj_data["history"]
                    ]
                )
            )
            scorecard["exit_status"] = ( # è®°å½•exit_statusçš„çŠ¶å†µ
                traj_data["info"]["exit_status"] # å¦‚æœinfoé‡Œæœ‰ï¼Œåˆ™å–infoçš„exit_status
                if "exit_status" in traj_data["info"]
                else "n/a" # å¦åˆ™å–n/a
            )

        # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†é¢„æµ‹ï¼Œå³é¢„æµ‹æ˜¯å¦ä¸ºç©º
        if p[KEY_PREDICTION] is None or p[KEY_PREDICTION].strip() == "":
            scorecard["statuses"].append("not_generated") # å¦‚æœé¢„æµ‹ç»“æœä¸ºç©ºï¼Œåˆ™å°†"not_generated"çŠ¶æ€æ·»åŠ åˆ°è¯„ä¼°ç»“æœçš„"statuses"å­—æ®µä¸­
            scorecards.append(scorecard)
            continue
        scorecard["statuses"].append("generated") # æœ‰é¢„æµ‹ï¼Œåˆ™å°†"generated"çŠ¶æ€æ·»åŠ åˆ°è¯„ä¼°ç»“æœçš„"statuses"å­—æ®µä¸­

        # Get log file
        log_file_name = f"{p[KEY_INSTANCE_ID]}.{p[KEY_MODEL]}.eval.log" # å½¢å¦‚'pvlib__pvlib-python-1072.vllm-llama3-70b__SWE-bench_Lite__default__t-0.00__p-0.95__c-2.00__install-1.eval.log'
        if args.log_suffix is not None:
            log_file_name = f"{p[KEY_INSTANCE_ID]}.{p[KEY_MODEL]}.{args.log_suffix}.eval.log"
        log_path = os.path.join(
            log_dir, log_file_name
        ) # å½¢å¦‚'/data1/zengzhengran/sweTrans_yang/SWE-agent/evaluation/log/pvlib__pvlib-python-1072.vllm-llama3-70b__SWE-bench_Lite__default__t-0.00__p-0.95__c-2.00__install-1.eval.log'
        if not os.path.exists(log_path):
            scorecard["statuses"].append("build_failure")
            scorecards.append(scorecard)
            continue

        # çŠ¶æ€æ˜ å°„å›¾ï¼ˆåªå…³å¿ƒ "APPLY_PATCH_PASS (pred)"ä¹‹åçš„å†…å®¹ï¼‰ï¼Œlogä¸­ä¿¡æ¯æ˜¯å¦å®Œæ•´
        eval_sm, found = get_logs_eval(log_path)

        # Check that the prediction generated
        if not found:
            scorecards.append(scorecard)
            continue
        scorecard["statuses"].append("applied")

        with open(log_path, "r") as f:
            log_contents = f.read()
            if INSTALL_FAIL in log_contents: # "install_fail"åœ¨logä¸­å‡ºç°ï¼Œåˆ™å°†"install_fail"çŠ¶æ€æ·»åŠ åˆ°è¯„ä¼°ç»“æœçš„"statuses"å­—æ®µä¸­
                scorecard["statuses"].append("install_fail")

        # Get resolution status
        report = get_eval_report(eval_sm, eval_refs[p[KEY_INSTANCE_ID]]) # ç»“åˆlogæ–‡ä»¶å’ŒåŸæ¥æ„å»ºçš„ï¼ˆFail-Pass, Pass-Passï¼‰å¯¹ï¼Œå¾—åˆ°è¯„ä¼°ç»“æœ
        scorecard["test_results"] = {
            "failure": {
                "FAIL_TO_PASS": report["FAIL_TO_PASS"]["failure"],
                "PASS_TO_PASS": report["PASS_TO_PASS"]["failure"],
            },
            "success": {
                "FAIL_TO_PASS": report["FAIL_TO_PASS"]["success"],
                "PASS_TO_PASS": report["PASS_TO_PASS"]["success"],
            }
        }
        resolution_status = get_resolution_status(report)
        scorecard["statuses"].append(resolution_status)

        try:
            diff_obj = PatchSet(p[KEY_PREDICTION])
            scorecard["patch_files"] = [
                x.path
                for x in diff_obj.modified_files
                + diff_obj.added_files
                + diff_obj.removed_files
            ]
            scorecard["patch_lines_add"] = sum([f.added for f in diff_obj])
            scorecard["patch_lines_del"] = sum([f.removed for f in diff_obj])
        except Exception as e:
            logger.info(f"[{p[KEY_INSTANCE_ID]}] Error parsing prediction diff: {e}")
            scorecard["patch_files"] = []
            scorecard["patch_lines_add"] = 0
            scorecard["patch_lines_del"] = 0
        scorecards.append(scorecard)

    # Save to summary, scorecard json
    path_scorecards = os.path.join(directory, "scorecards.json") # å½¢å¦‚'/data1/zengzhengran/sweTrans_yang/SWE-agent/trajectories/zengzhengran/vllm-llama3-70b__SWE-bench_Lite__default__t-0.00__p-0.95__c-2.00__install-1/scorecards.json'
    with open(path_scorecards, "w") as f:
        json.dump(scorecards, fp=f, indent=2)
    logger.info(f"- Wrote per-instance scorecards to {path_scorecards}")

    # Get results and write to file
    logger.info(f"Reference Report:")
    report = get_model_report(directory_name, pred_path_orig, swe_bench_tasks, log_dir)
    for k, v in report.items():
        logger.info(f"- {k}: {len(v)}")

    path_results = os.path.join(directory, "results.json") # å½¢å¦‚'/data1/zengzhengran/sweTrans_yang/SWE-agent/trajectories/zengzhengran/vllm-llama3-70b__SWE-bench_Lite__default__t-0.00__p-0.95__c-2.00__install-1/results.json'
    with open(path_results, "w") as f:
        json.dump(report, f, indent=2)
    logger.info(f"- Wrote summary of run to {path_results}")

if __name__ == "__main__":
    # Parse arguments
    parser = argparse.ArgumentParser()
    parser.add_argument("--predictions_path", type=str, help="Path to predictions file (.jsonl)", required=True,)
    parser.add_argument("--log_dir", type=str, help="Path to log directory", required=True)
    parser.add_argument("--swe_bench_tasks", type=str, help="Path to SWE-bench task instances file", required=True,)
    parser.add_argument("--testbed", type=str, help="Path to testbed directory", required=True)
    parser.add_argument("--skip_existing", action="store_true", help="(Optional) Skip existing logs")
    parser.add_argument("--timeout", type=int, help="(Optional) Timeout in seconds (default: 900)", default=900,)
    parser.add_argument("--verbose", action="store_true", help="(Optional) Verbose mode")
    parser.add_argument("--conda_link", default=None, type=str, help="(Optional) URL to conda installation to use")
    parser.add_argument("--log_suffix", default=None, type=str, help="(Optional) Log suffix")
    parser.add_argument("--num_processes", default=-1, type=int, help="Num processes")
    parser.add_argument("--path_conda", type=str, default=None, help="(Optional) Path to miniconda3 or anaconda installation")
    parser.add_argument("--instance_filter", type=str, default=None, help="(Optional) Number of workers")
    
    args = parser.parse_args()
    main(**vars(args))

